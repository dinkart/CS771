{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01aa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a85c88",
   "metadata": {},
   "source": [
    "# Question 1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce48e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(gradient, init_, learn_rate, n_iter = 50, tol = 1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate*gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67111b79",
   "metadata": {},
   "source": [
    "Gradient of $(x^2 + 3x + 4)$  is  $(2x + 3)$  \n",
    "Gradient of $(x^4 - 3x^2 + 2x)$  is  $(4x^3 - 6x + 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caeb9e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima of x^2 + 3x + 4  : -1.5\n",
      "Minima of x^4 - 3x^2 + 2x  : -1.366\n"
     ]
    }
   ],
   "source": [
    "min1 = gradient_decent(lambda v:2*v + 3, 1.0, 0.2)\n",
    "print(\"Minima of x^2 + 3x + 4  : \" + str(min1))\n",
    "min2 = gradient_decent(lambda v:4*v*v*v - 6*v + 2, 2, 0.1)\n",
    "print(\"Minima of x^4 - 3x^2 + 2x  : \" + str(min2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c8308",
   "metadata": {},
   "source": [
    "# Question 1(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503ac134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w, X, y):\n",
    "    a, b = w\n",
    "    y_current = a * X + b\n",
    "    N = len(X)\n",
    "    a = -(2/N) * sum(X * (y - y_current))\n",
    "    b = -(2/N) * sum(y - y_current)\n",
    "    return np.array([a,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e00da",
   "metadata": {},
   "source": [
    "gradient() function is taking three parameter :  \n",
    "w -> Touple of (a, b)  \n",
    "X -> Values of x  \n",
    "y -> actual values of y\n",
    "\n",
    "It is returning a array of (a, b) which are the gradients of y = ax + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc826e8",
   "metadata": {},
   "source": [
    "# Question 1(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc9acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data\n",
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5\n",
    "res = 1.5 * np.random.randn(10000)\n",
    "y = 2 + 0.3*X + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e94622bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Little modified gradient decent function for linear regression\n",
    "def gradient_decent_lreg(gradient, init_, learn_rate, n_iter = 50, tol = 1e-06):\n",
    "    x, X, y = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate*gradient(x, X, y)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28db59d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 0.295    b = 2.023\n"
     ]
    }
   ],
   "source": [
    "a , b = 0, 0\n",
    "init = [(a, b), X, y]\n",
    "a,b = gradient_decent_lreg(gradient, init, 0.01, 1000)\n",
    "print(\"a = \" + str(a) + \"    b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ed253",
   "metadata": {},
   "source": [
    "# Question 1(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de78f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To divide the data if random mini batches of size B\n",
    "def create_mini_batches(X, y, B):\n",
    "    mini_batches = []\n",
    "    data = np.stack((X, y), axis = 1)\n",
    "    np.random.shuffle(data)\n",
    "    n = len(X) // B\n",
    "    for i in range(n):\n",
    "        batch = data[i*B : (i+1)*B]\n",
    "        mini_batches.append((batch[:,0], batch[:,1]))\n",
    "    if len(X) % B != 0:\n",
    "        batch = data[(i+1)*B:]\n",
    "        mini_batches.append((batch[:,0], batch[:,1]))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64bfeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_decent(gradient, init_, learn_rate, n_iter = 50, tol = 1e-06):\n",
    "    x, X, y, B = init_\n",
    "    batches = create_mini_batches(X, y, B)\n",
    "    for _ in range(n_iter):\n",
    "        bidx = np.random.randint(len(batches))\n",
    "        X_new, y_new = batches[bidx]\n",
    "        delta = -learn_rate*gradient(x, X_new, y_new)\n",
    "        if np.any(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfda0b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : 0.304 b : 2.019\n"
     ]
    }
   ],
   "source": [
    "w = [0, 0]\n",
    "B = 64\n",
    "init = [w, X, y, B]\n",
    "a,b = minibatch_gradient_decent(gradient, init, 0.01, 1000)\n",
    "print(\"a : \" + str(a) + \" b : \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b4d12",
   "metadata": {},
   "source": [
    "# Question 1(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ebed992",
   "metadata": {},
   "source": [
    "Time performance of Stochastic Gradient Descent(SGD) vs Mini-batch Stochastic Gradient Descent(MSGD) v Gradient Descent(GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a69b4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.28125 secs\n",
      "a : 0.029 b : 1.828\n"
     ]
    }
   ],
   "source": [
    "#SGD      -> Mini batch with batch size = 1\n",
    "w = [0, 0]\n",
    "B = 1\n",
    "init = [w, X, y, B]\n",
    "tic = t.process_time()\n",
    "a,b = minibatch_gradient_decent(gradient, init, 0.01, 10000)\n",
    "toc = t.process_time()\n",
    "print(\"It took \" + str(toc - tic) + \" secs\")\n",
    "print(\"a : \" + str(a) + \" b : \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79b1a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.109375 secs\n",
      "a : 0.307 b : 1.981\n"
     ]
    }
   ],
   "source": [
    "#MSGD\n",
    "w = [0, 0]\n",
    "B = 1024\n",
    "init = [w, X, y, B]\n",
    "tic = t.process_time()\n",
    "a,b = minibatch_gradient_decent(gradient, init, 0.01, 10000)\n",
    "toc = t.process_time()\n",
    "print(\"It took \" + str(toc - tic) + \" secs\")\n",
    "print(\"a : \" + str(a) + \" b : \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89193174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.28125 secs\n",
      "a : 0.295 b : 2.023\n"
     ]
    }
   ],
   "source": [
    "#GD\n",
    "w = [0, 0]\n",
    "init = [w, X, y]\n",
    "tic = t.process_time()\n",
    "a,b = gradient_decent_lreg(gradient, init, 0.01, 10000)\n",
    "toc = t.process_time()\n",
    "print(\"It took \" + str(toc - tic) + \" secs\")\n",
    "print(\"a : \" + str(a) + \" b : \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b4690",
   "metadata": {},
   "source": [
    "So, we can see SGD and MSGD are taking almost same time and GD is taking more time for the given data in same no of iterations. So, each iteration in SGD and MSGD is much faster than GD. But GD is more precise, where are SGD is least precise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484467d9",
   "metadata": {},
   "source": [
    "For optimal minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3871135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.0625 secs\n",
      "a : 0.295 b : 2.026\n",
      "It took 0.03125 secs\n",
      "a : 0.328 b : 2.101\n",
      "It took 0.046875 secs\n",
      "a : 0.284 b : 1.997\n",
      "It took 0.03125 secs\n",
      "a : 0.33 b : 1.97\n",
      "It took 0.046875 secs\n",
      "a : 0.275 b : 2.054\n",
      "It took 0.03125 secs\n",
      "a : 0.272 b : 2.016\n",
      "It took 0.03125 secs\n",
      "a : 0.282 b : 2.033\n",
      "It took 0.0625 secs\n",
      "a : 0.339 b : 1.983\n",
      "It took 0.03125 secs\n",
      "a : 0.299 b : 2.015\n",
      "It took 0.03125 secs\n",
      "a : 0.259 b : 2.016\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "for _ in range(10):\n",
    "    w = [0, 0]\n",
    "    init = [w, X, y, B]\n",
    "    tic = t.process_time()\n",
    "    a,b = minibatch_gradient_decent(gradient, init, 0.01, 1000)\n",
    "    toc = t.process_time()\n",
    "    print(\"It took \" + str(toc - tic) + \" secs\")\n",
    "    print(\"a : \" + str(a) + \" b : \" + str(b))\n",
    "    b *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d57a0",
   "metadata": {},
   "source": [
    "As we see, increasing batch size increases accuracy, but time per iteration also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada54e4",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98502a29",
   "metadata": {},
   "source": [
    "$i) p(Cold,Fever) = p(Fever|Cold) * p(Cold) = 0.02 * 0.307 = 0.00614$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844ccc4",
   "metadata": {},
   "source": [
    "$ii) p(Cough|Cold) = \\frac{p(Cough,Cold)}{p(Cough)} = \\frac{p(Cough, Cold, Lung Disease) + p(Cough, Cold, !Lung Disease)}{p(Cough, Cold, Lung Disease) + p(Cough, Cold, !Lung Disease) + p(Cough, !Cold, Lung Disease) + p(Cough, !Cold, !Lung Disease)}$  \n",
    "  \n",
    "$ p(Lung Disease)= 0.1009 * 0.2 + 0.009*0.8 = 0.02098 $  \n",
    "$ p(!Lung Disease)= 0.8991 * 0.2 + 0.999*0.8 = 0.97902 $  \n",
    "$ p(Cough, Cold, Lung Disease) = 0.7525 * 0.02 * 0.02098 = 0.0003 $  \n",
    "$ p(Cough, Cold, !Lung Disease) = 0.505 * 0.02 * 0.97902 = 0.0098 $  \n",
    "$ p(Cough, !Cold, Lung Disease) = 0.505 * 0.98 * 0.02098 = 0.0103 $  \n",
    "$ p(Cough, !Cold, !Lung Disease) = 0.01 * 0.98 * 0.97902 = 0.0095 $  \n",
    "  \n",
    "Using these values,  \n",
    "$ p(Cough|Cold) = \\frac{0.0003 + 0.0098}{0.0003 + 0.0098 + 0.0103 + 0.0095}$  \n",
    "  \n",
    "$p(Cough|Cold) = 0.3377 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed34fd9",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f194a",
   "metadata": {},
   "source": [
    "Multinomial distribution : Generalized Binomial distribution  \n",
    "It models the probability of counts for rolling a K-sided die N times  \n",
    "$\\begin{align}\n",
    "p(\\mathbf{y}) &= {{n}\\choose{x_1, ..., x_k}}\\prod_{i=1}^k y_i^{x_i} \\\\\n",
    "&= C \\prod_{i=1}^k {y_i^{x_i}}\n",
    "\\end{align}$  \n",
    "Log Likelihood:  \n",
    "$\\begin{align}\n",
    "l(\\mathbf{y}) = \\log p(\\mathbf{y}) \n",
    "&= \\log \\bigg( C \\prod_{i=1}^k {y_i^{x_i}} \\bigg)\\\\\n",
    "&= \\log C + \\log \\prod_{i=1}^k {y_i^{x_i}} \\\\\n",
    "\\end{align}$  \n",
    "$\\begin{align}\n",
    "y_k = 1 - \\sum_{i = 1}^{k-1} y_i\n",
    "\\end{align}$  \n",
    "$\\begin{align}\n",
    "l(\\mathbf{y}) = \\log p(\\mathbf{y}) \n",
    "&= \\log C + \\log \\left (\\prod_{i=1}^{k-1} {y_i^{x_i}} \\right ) {y_k^{x_k}}\\\\\n",
    "&= \\log C + \\log \\left (\\prod_{i=1}^{k-1} {y_i^{x_i}} \\right ) \\left ( {1 - \\sum_{i=1}^{k-1} y_i} \\right )^{x_k} \\\\\n",
    "&= \\log C + \\sum_{i=1}^{k-1} {x_i \\log y_i} +  x_k \\log \\left ( {1 - \\sum_{i=1}^{k-1} y_i} \\right ) \\\\\n",
    "\\end{align}$  \n",
    "$\\begin{align}\n",
    "\\mathbf{\\frac{\\partial l}{\\partial x_j}} &= 0\n",
    "\\Rightarrow0 + \\frac{x_j}{y_j} + \\frac{x_k}{\\left ( 1 - \\sum_{i=1}^{k-1} y_i \\right )}\\left ( -1 \\right ) = 0\\\\\n",
    "&\\Rightarrow \\frac{x_j}{y_j} = \\frac{x_k}{\\left ( 1 - \\sum_{i=1}^{k-1} y_i \\right)} = \\frac{x_k}{y_k}\\\\\n",
    "&\\Rightarrow y_j = y_k*\\frac{x_j}{x_k}\n",
    "\\end{align}$  \n",
    "Now, we know  \n",
    "$\\begin{align}\n",
    "\\mathbf{y_1 + y_2 + .... + y_k} = 1\n",
    "&\\Rightarrow\\left [\\frac{x_1}{x_k} + \\frac{x_2}{x_k} + ... + \\frac{x_k}{x_k}\\right ]y_k = 1\\\\\n",
    "&\\Rightarrow y_k = \\frac{x_k}{\\sum_{i = 1}^k x_i}\n",
    "\\end{align}$  \n",
    "  \n",
    "$y_k$ is the MLE for k-sided multinomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09e4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
